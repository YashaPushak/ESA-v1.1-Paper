% PLEASE USE THIS FILE AS A TEMPLATE
% Check file iosart2x.tex for more examples

% add. options: [seceqn,secthm,crcready,onecolumn]
\documentclass[aic]{iosart2x}

\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{array}
%\usepackage{url}
\usepackage{multirow}
\usepackage{graphicx}
%\usepackage{caption}
%\usepackage{subcaption}
%\captionsetup{compatibility=false}
\usepackage{verbatim}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,arrows.meta}

%\usepackage{babel}
\usepackage{listings}
\renewcommand{\lstlistingname}{Listing}

%\usepackage{dcolumn}
%\usepackage{endnotes}

%%%%%%%%%%% Put your definitions here

\newcommand{\eg}{\emph{e.g.}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\etal}{\emph{et al.}}
\newcommand{\todo}[1]{\textcolor{red}{\bf [TODO: #1]}}
%\renewcommand{\todo}[1]{}
\newcommand{\yp}[1]{\textcolor{blue}{#1}}
\newcommand{\note}[1]{\textcolor{green}{[Note: #1]}}
\newcommand{\append}[1]{\textcolor{violet}{[Appendix?] #1}}
\newcommand{\zm}[1]{#1}
\newcommand{\hide}[1]{}
\newcommand{\figtitle}[1]{\textbf{#1}\par\medskip}
\renewcommand{\figtitle}[1]{}

%%%%%%%%%%% End of definitions

\pubyear{0000}
\volume{0}
\firstpage{1}
\lastpage{1}

\begin{document}



\begin{frontmatter}

%\pretitle{}
\title{Empirical Scaling Analyzer: \newline An Automated System for Empirical Analysis of Performance Scaling}
\runningtitle{Empirical Scaling Analyzer}

% For one author:
%\author{\inits{N.}\fnms{Name1} \snm{Surname1}\ead[label=e1]{first@somewhere.com}}
%\address{Department first, \orgname{University or Company name},
%Abbreviate US states, \cny{Country}\printead[presep={\\}]{e1}}
%\runningauthor{N. Surname1}

% Two or more authors:
\author[A]{\inits{Y.}\fnms{Yasha} \snm{Pushak}\ead[label=e1]{ypushak@cs.ubc.ca}},
\author[A]{\inits{Z.}\fnms{Zongxu} \snm{Mu}\ead[label=e2]{zongxumu@cs.ubc.ca}}
and
\author[B,A]{\inits{H.H.}\fnms{Holger H.} \snm{Hoos}\ead[label=e3]{hh@liacs.nl}%
\thanks{Corresponding author. \printead{e3}.}}
\runningauthor{Y. Pushak et al.}
\address[A]{Department of Computer Science, \orgname{The University of British Columbia},
BC, \cny{Canada}\printead[presep={\\}]{e1}}
\address[B]{LIACS, \orgname{Universiteit Leiden}, \cny{The Netherlands}\printead[presep={\\}]{e2,e3}}

\begin{abstract}
The time complexity of algorithms, \ie, the scaling of the time required for solving a problem instance as a function of instance size, is of key interest in theoretical computer science and practical applications. In this work, we present a fully automated tool -- Empirical Scaling Analyzer (ESA) -- for performing sophisticated and detailed empirical scaling analyses. The methodological approach underlying ESA is based on a combination of automatic function fitting and bootstrap sampling; previous versions of the methodology have been used in prior work to characterize the empirical scaling behaviour of several prominent, high-performance SAT and TSP solvers. ESA is applicable to any algorithm or system, as long as running time data can be collected on sets of problem instances of various sizes. We present results from rigorous stress-testing to critically assess ESA on scenarios with challenging characteristics. 
%HH: added:
We also give an overview of empirical scaling results obtained using ESA.
%YP: Currently 151 words -- just over the 150 word limit for AI Communications. 
\end{abstract}

\begin{keyword}
\kwd{Empirical scaling analysis}
\kwd{Running time scaling}
\end{keyword}

\end{frontmatter}

%%%%%%%%%%% The article body starts:


\section{Introduction}
%Introduction of scaling of runtimes for algorithms and the empirical analysis.
In theoretical computer science, time complexity is one of the most
prominent concepts arising in the analysis of problems and algorithms.
The time complexity of an algorithm describes the time required for
solving a problem instance as a function of instance size and is traditionally
studied by means of theoretical analysis. For instance, the
Boolean satisfiability problem (SAT) and the travelling salesman problem
(TSP) are two prominent $\mathcal{NP}$-hard problems, for which the best algorithms
currently known have exponential time complexity in the worst case. 
%HH: added:
However, worst-case behaviour may be encountered rarely or never at all in practical 
situations.
Therefore, empirical analysis of time complexity has seen increasing interest, because it permits
predicting the running times of high-performance algorithms in practice
and may also provide useful insights into their behaviour \cite{Kun02,SubDes05,AguEtAl07}.

%Description of the methodology proposed in \cite{hoos2009bootstrap}.

Very few methods exist for performing empirical running time scaling analysis that handle noise and the stochastic behaviour of algorithms in a principled, statistical way~\cite{Hoo09}. 
Common practice among empirically oriented algorithm researchers is to perform relatively small numbers of algorithm runs while varying problem instance size, with some of the more advanced methods taking means over independent runs of the algorithm on the same input instances to reduce the variance in observations~\cite{Sun09}. 
In some cases, the mean over tens or hundreds of runs on problems of the same size are plotted for varying instance sizes, and these points are compared against each other for two competing algorithms to show that one out-performs the other. 
In slightly more advanced work, standard least squares regression and curve-fitting procedures are used to fit and subsequently visualize  trend lines~\cite{ZapHau12}. 
An improvement to this practice was introduced by McGeoch \etal~\cite{McgEtAl02}, who described and evaluated several prototype methods for fitting and bounding empirical running time data with polynomial models. 

Somewhat related to our work are methods designed to perform algorithm profiling that can automatically extract notions of problem instance size and algorithm running time; however, even these rely on the simple methods we have described above~\cite{ZapHau12,CopEtAl12,CopEtAl14}. 
Ultimately, the goal of performing empirical running time scaling analysis is to obtain estimates or bounds on how well we can expect an algorithm to perform for larger problem instance sizes than those used to perform the analysis.
However, neither the work by McGeoch \etal~\cite{McgEtAl02} nor simple curve-fitting procedures address the  question how much faith we should have in the accuracy of the performance extrapolations obtained from empirical models of performance scaling.

A significant advance in empirical running time scaling analysis was achieved by Hoos~\cite{Hoo09}, who introduced a new methodology to address both previously ignored or poorly handled challenges: running time variances and extrapolation accuracy.
They do this by using a complex bootstrap sampling procedure to handle variability in running time in a statistically meaningful way, and by assessing extrapolation accuracy using a set of ``challenge'' instances withheld during the model fitting.  
In our work presented here, we extend this methodology and introduce it in the form of a fully automated tool: the Empirical Scaling Analyzer (ESA). 
ESA takes an input file providing running time data for an algorithm (referred to as target algorithm hereafter), as well as other optional files to configure ESA. 
ESA is not limited to fitting and assessing a single scaling model, but can deal with multiple models simultaneously -- in other words, once data collection is finished, a user can collate all running time data into a file, feed it into ESA and obtain results from the scaling analysis using several parametric models. 
The results are presented in a technical report, which contains easy-to-read tables and figures for the scaling of the target algorithm. 
ESA also automatically interprets the results and assesses whether a model describes the running data well, using an advanced decision model newly developed here.

Many state-of-the-art solvers for challenging computational problems are highly heuristic, so their performance scaling can typically not be characterized with any reasonable precision using theoretical approaches.
In this situation, empirical scaling analysis, based on advanced statistical techniques, is the method of choice. 
ESA makes such an approach broadly and conveniently accessible. 
For example, the methodology underlying ESA has previously been applied to state-of-the-art local search algorithms for Euclidean TSP instances~\cite{DubEtAl15}, and we applied a prototype of ESA to study the empirical scaling of high-performance SAT solvers~\cite{MuHoo15}, which we later extended to two classes of 4-SAT instances~\cite{Mu15}. 
We have also used earlier versions of ESA to perform empirical analysis of two inexact TSP solvers and to investigate the impact of automated algorithm configuration on their empirical running time scaling~\cite{MuEtAl16,Mu15}. 
More recently, we have used ESA to extend the analysis of these cutting-edge inexact TSP algorithms to compare their scaling with that of a state-of-the-art exact TSP algorithm~\cite{MuEtAl17}.

We believe that ESA will prove to be useful for other researchers who want to study the empirical time complexity of other algorithms. ESA is available as an easy-to-use on-line service at \url{www.cs.ubc.ca/labs/beta/Projects/ESA} and can also be downloaded and installed locally as a command-line tool with additional functionality.

Our work presented in the following makes two main contributions: 
we present ESA (see Section~\ref{sec:Implementation and Use}), a fully-automated implementation of an advanced empirical running time scaling analysis methodology~\cite{Hoo09} (see Section~\ref{sec:method} for a summary of the methodology and our improvements to it); and we summarize the results of performing rigorous experiments with ESA on challenging scenarios (see Sections~\ref{sec:Stress Testing} and~\ref{sec:Lower Order Terms}). 
Improvements to ESA over an earlier, preliminary version include a nested bootstrap sampling procedure for randomized target algorithms and a novel method for automatically assessing the quality of fitted models (described in Section~\ref{sec:auto-interpretation}). 
To design challenging benchmarking tests for ESA, we also introduce a novel method for artificially generating realistic running time data (see Section~\ref{sec:AA}). 
We further discuss several successful applications of the methodology underlying ESA in previous work (see Section~\ref{sec:Successful Applications}), demonstrating its power and ability to provide meaningful insights in a diverse set of applications. 
Finally, we provide some general conclusions and briefly discuss future extensions to ESA (see Section~\ref{sec:Conclusion}).

% *** HH to continue from here


\section{Methodology}
\label{sec:method}

% advanced empirical scaling analysis
The methodology underlying ESA is illustrated in Figure~\ref{fig:methodology} \cite{Hoo09}.
\begin{figure}[t]
\noindent \centering{}\tikzstyle{decision} = [diamond, draw, fill=blue!20, text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{smallblock} = [rectangle, draw, fill=blue!20, text width=3em, text centered, rounded corners, minimum height=2em]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, text width=6em, text centered, rounded corners, minimum height=4em]
\tikzstyle{longblock} = [rectangle, draw, fill=blue!20, text width=14em, text centered, rounded corners, minimum height=4em]
\tikzstyle{cloud} = [draw, ellipse, fill=red!20, text width=4.5em, text centered, node distance=3cm, minimum height=2em]
\tikzstyle{data} = [draw, ellipse, fill=green!20, text width=4.5em, text centered, node distance=3cm, minimum height=2em]
\tikzstyle{line} = [draw, -{Latex[length=2.5mm,width=1mm]}]

\scalebox{0.7}{
\begin{tikzpicture}[node distance = 7em, auto]
\node [data] (input) {solver running times};
\node [block, right=2.5em of input] (fit) {fit parametric models};
\node [block, right=2.5em of fit] (challenge) {challenge by extrapolation};
\node [cloud, below=2.5em of input] (report) {technical report};
\node [longblock, right=3em of report] (bootstrap) {use bootstrap re-sampling for further assessment};

\path [line] (input) -- (fit);
\path [line] (fit) -- (challenge);
\path [line] (challenge) -- (bootstrap);
\path [line] (bootstrap) -- (report);
\end{tikzpicture}
}
\caption{Empirical scaling analysis approach underlying ESA.}\label{fig:methodology}
\end{figure}
Scaling models are fitted using the Levenberg-Marquardt algorithm, a prominent numerical optimization procedure,
on a set of running time data called the support set. The scaling models thus obtained are challenged using previously unseen running time data for larger problem instances (called the challenge data). Then, bootstrap sampling is used to quantify the confidence we can have in the scaling models. In particular, we re-sample, with replacement, the running times for each support instance size. We use these bootstrap samples to obtain a distribution of fitted models for each parametric model. These model distributions are then used to generate a set of distribution of predictions for the challenge instance sizes, one for each parametric model. Bootstrap percentile confidence intervals (for a given confidence level $\alpha$) are then computed for each challenge instance size and for each parametric scaling model. Finally, these intervals are used to determine whether or not a parametric model should be rejected at confidence level $\alpha$.

In recent work~\cite{MuHoo15}, we extended the original methodology by noting that observed running time statistics for challenge data are also based on measurements on sets of instances, so we now calculate bootstrap confidence intervals for those, in addition to the point estimates used in the original approach. This way, we capture dependency of these statistics on the underlying sample of instances.

Here, we further extend the original methodology to accurately handle variance between independent runs of randomized algorithms by using a nested bootstrap sampling procedure. In this procedure we first obtain statistics, \eg{}, medians, of bootstrap samples for each individual problem instance. Then we randomly select one of these per-instance statistics each time that instance is selected for inclusion in a bootstrap sample at the instance set level. 


%\subsection{Automated Interpretation of Scaling Results}
\label{sec:auto-interpretation}

ESA automatically generates interpretations for the results of the scaling analysis. This is done by assessing how well a model fits the given challenge data, based on the percentage of challenge instance sizes for which the model predicts the corresponding running times reasonably accurately. If a model produces good predictions for most challenge sizes, then the model should be accepted as a good fit. Technically, we define a very good prediction, or a \emph{strongly consistent prediction}, as one for which the predicted bootstrap confidence interval contains the observed challenge confidence interval, and we define a good prediction, or a \emph{weakly consistent prediction}, as one for which the predicted and observed bootstrap confidence intervals are overlapping.

Our interpretation procedure especially emphasizes the challenge points for larger instance sizes, as those provide more information regarding whether the model predictions scale with instance size. We designed the procedure based on extensive experiments with several algorithms (both real and artificial), to produce statements similar to those that would be made by an expert upon viewing a plot of the fitted models. Each statement is determined using the number of model predictions that are strongly and weakly consistent with the observations, so the procedure is best viewed as a heuristic grounded in a statistical method. The detailed criteria are as follows:
%HH: I've made some changes in the following, in light of the fact that we technically define consistency etc. for predictions, not for intervals (although we do it based on confidence intervals) - DONE: check
\begin{itemize}
\item \textbf{very good fit}: the model predicts very well for most of the challenge sizes; more precisely, $\geq 90\%$ of the predictions for challenge sizes are strongly consistent, or $\geq 90\%$ of the predictions for the larger half of the challenge sizes are strongly consistent and $ \geq 90\%$ of all of the predictions for all challenge sizes are weakly consistent;

\item \textbf{fair fit}: the model predicts well for most of the challenge sizes; more precisely, $\geq 90\%$ of the predictions for challenge sizes or $\geq 90\%$ of predictions for the larger half of the challenge sizes are weakly consistent;

\item \textbf{tends to over-/under-estimate}: the model predictions are over-/under-estimates or are weakly consistent with observed running time data for most of the challenge instance sizes; more precisely, $> 10\%$ of the confidence intervals for predictions on challenge instance sizes are disjoint from the confidence intervals for observed running time data and $\geq 90\%$ of the predicted intervals are above/below or are consistent with the observed intervals;

\item \textbf{over-/under-estimate}: the model predictions are over-/under-estimates of a large percentage of the challenge sizes; more precisely, $\geq 70\%$ of the confidence intervals for predictions on all challenge instance sizes or $\geq 70\%$ of those on the larger half of the challenge sizes are above/below the observed intervals.

%HH: Missing: some explanation why these classes were defined in this way - DONE: add brief explanation

\end{itemize}
These criteria are combined into the fully automated interpretation procedure illustrated in Figure \ref{fig:ESA-auto-interpretation}. Note that when medians (or other statistics) are not definitely known (due to instances with unknown running times), we create bootstrap confidence intervals for the medians that combine both sources of uncertainty, and we compare these intervals against those for the predicted running times. To be more precise, we determine confidence intervals combining both sources of variability (due to differences between problem instances of a given size and due to randomness in the algorithm) using an optimistic-pessimistic strategy, whereby we treat an unknown running time as zero in a lower bound and as infinity in an upper bound. Then, we say a bootstrap confidence interval $I_o$ for observed running time on a given challenge instance size is below the corresponding confidence interval $I_p$ for predicted running time, if the upper bound of $I_o$ is smaller than the lower bound of $I_p$.

\begin{figure*}[t]
\noindent \begin{centering}
\vspace*{-3mm}
{\small{}\tikzstyle{decision} = [diamond, draw, fill=blue!20, text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{smallblock} = [rectangle, draw, fill=blue!20, text width=3em, text centered, rounded corners, minimum height=2em]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, text width=6em, text centered, rounded corners, minimum height=4em]
\tikzstyle{longblock} = [rectangle, draw, fill=blue!20, text width=14em, text centered, rounded corners, minimum height=4em]
\tikzstyle{cloud} = [draw, ellipse, fill=red!20, text width=4.5em, text centered, node distance=3cm, minimum height=2em]
\tikzstyle{longcloud} = [draw, ellipse, fill=red!20, text width=7.25em, text centered, node distance=3cm, minimum height=2em]
\tikzstyle{data} = [draw, ellipse, fill=green!20, text width=4.5em, text centered, node distance=3cm, minimum height=2em]
\tikzstyle{line} = [draw, -{Latex[length=2.5mm,width=1mm]}]

\scalebox{0.7}{
\begin{tikzpicture}[node distance = 7em, auto]

\node [data] (start) {models \& data};
\node [decision, right=4em of start] (goodfit) {very good fit?};
\node [decision, right=7em of goodfit] (fairfit) {fair fit?};
\node [decision, right=4.75em of fairfit] (underEstimate) {under-estimate?};

\node [cloud, below=4.5em of start] (fitWellRes) {``fits very well''};
\node [cloud, right=3.5em of fitWellRes] (tendToFitWellRes) {``tends to fit well''};
\node [cloud, right=5.5em of tendToFitWellRes] (underEstimateRes) {``under-estimate''};
\node [decision, right=3.75em of underEstimateRes] (overEstimate) {over-estimate?};

\node [cloud, below=4em of fitWellRes] (notFitWellRes) {``does not fit well''};
\node [decision, right=4em of notFitWellRes] (tendOver) {tends over?};
\node [decision, right=6em of tendOver] (tendUnder) {tends under?};
\node [cloud, right=4em of tendUnder] (overEstimateRes) {``over-estimate''};

\node [longcloud, below=3.5em of tendOver] (tendOverEstimateRes) {``tends to over-estimate''};
\node [longcloud, right=1.25em of tendOverEstimateRes] (tendUnderEstimateRes) {``tends to under-estimate''};

\path [line] (start) -- (goodfit);
\path [line] (goodfit) -- node {no} (fairfit);
\path [line] (goodfit) -- node {yes} (fitWellRes);
\path [line] (fairfit) -- node {yes} (tendToFitWellRes);
\path [line] (fairfit) -- node {no} (underEstimate);
\path [line] (underEstimate) -- node {yes} (underEstimateRes);
\path [line] (underEstimate) -- node {no} (overEstimate);
\path [line] (overEstimate) -- node {yes} (overEstimateRes);
\path [line] (overEstimate) -- node {no} (tendUnder);
\path [line] (tendUnder) -- node {yes} (tendUnderEstimateRes);
\path [line] (tendUnder) -- node {no} (tendOver);
\path [line] (tendOver) -- node {yes} (tendOverEstimateRes);
\path [line] (tendOver) -- node {no} (notFitWellRes);

%\path [line] (overEstimate) -- node {yes} (overEstimateRes);
%\path [line] (overEstimate) -- node {no} (underEstimate);
%\path [line] (underEstimate) -- node {yes} (underEstimateRes);
%\path [line] (underEstimate) -- node {no} (notFitWellRes);
\end{tikzpicture}}
}

\par\end{centering}{\small \par}

\caption{Procedure used by ESA for automatic interpretation of scaling analysis results. Details on the conditions are provided in the main text.}\label{fig:ESA-auto-interpretation}
\end{figure*}


\section{Running ESA}
\label{sec:Implementation and Use}

ESA implements the methodology described in Section~\ref{sec:method} in Python 2.7, also making use of gnuplot and \LaTeX{} to automatically generate and compile an easy-to-read technical report (in the form of a PDF file) containing detailed empirical scaling analysis results presented in tables and figures and their interpretation using our new procedure outline in Section~\ref{sec:auto-interpretation}

%\subsection{Input}
\label{sec:ESA-Input}

To perform scaling analysis, ESA requires input data containing the sizes of the instances studied and the running times of the target algorithm on those instances. 
The user may also specify the number of instances for some sizes; if there are fewer entries for a given instance size than specified explicitly, ESA will treat the missing entries as instances with unknown running times. 
An example for such data is found in a recent study by Dubois \etal~\cite{DubEtAl15}, in the context of analyzing the scaling behaviour of two inexact TSP algorithms, where the running times of some instances were unknown due to unknown optimal tour lengths. 
An excerpt of an input file for ESA is shown in Figure~\ref{fig:Excerpt-ESA-input}. 
In this example, multiple running times are provided for each instance, each of which corresponds to an independent run of the target algorithm on the specified instance. The user is required to include at least one column with running times; however, any number of additional columns may be appended to the file to add additional independent runs per instance. 

\begin{figure}[t]
\begin{lstlisting}[basicstyle={\scriptsize\ttfamily}]
#instance, size, datum (running time), datum, datum
portgen-500-1000.tsp, 500, 2.3, 2.54, 2.09
portgen-500-100.tsp, 500, 2.58, 2.28, 2.71
portgen-500-101.tsp, 500, 2.36, 2.44, 2.53
...
portgen-600-1000.tsp, 600, 3.4, 3.54, 4.01
...
portgen-4500-10.tsp, 4500, 727.68, inf, 801.32
portgen-4500-11.tsp, 4500, inf, inf, inf
...
#instances,4000,100
#instances,4500,100
\end{lstlisting}
\vspace*{-3mm}
\caption{Example input file for ESA, where ``...'' represents omitted lines analogous to those shown.}\label{fig:Excerpt-ESA-input}
\end{figure}

ESA also takes as input a configuration file, containing details on the target algorithm (algName), the instance distribution (instName), the number of support instance sizes (numTrainingData), \emph{etc.}  
Each line of this file specifies one parameter setting in the format of  ``\lstinline[basicstyle={\ttfamily}]!name : value!''.

There are a number of other files that a user may supply, including: a file specifying the models to be fitted, a {\LaTeX} template specifying the content and format of the output report, and gnuplot templates specifying the format of the plots appearing in the report. 
% (If any of these input files are not supplied, ESA will use default the default file(s) distributed with the code))
% HH: <- IMO, not needed here (this is more (suitable for a readme or user manual)
The first of these is needed, because ESA supports customized models, as long as the models are supported by python (including the math and numpy packages) and gnuplot. 
Each line of this file specifies one parametric scaling model,  including the model name (\eg{}, Exponential), the number of parameters (\eg{}, 2), {\LaTeX}, python and gnuplot expressions for the model, as well as default values for the fitting parameters. 
In the mathematical expressions for the models, $x$ represents the instance size, while model parameters are written as $@@a@@, @@b@@,$ \emph{etc.}


ESA comes with a default {\LaTeX} template for the report containing the results of the scaling analysis. This template can be customized easily by anyone with working knowledge of \LaTeX. 
Dynamic elements are enclosed in ``@@'' in the template; \eg{}, the target algorithm name specified in the configuration file is referenced as ``@@algName@@''. 
Users of ESA can also modify the formatting of the plots used for graphically presenting scaling analysis results, by editing the default template gnuplot script, \eg{}, to obtain log-log or semi-log plots.

%\subsection{Output}
\label{sec:ESA-Output}

 Here, we illustrate some examples of ESA output from our analysis on EAX~\cite{NagKob13}, a state-of-the-art inexact TSP solver based on an evolutionary algorithm with a special edge assembly crossover operator~\cite{Nag97}. The tables and figures include:
 %HH: Why don't we show example for all tables and figures? I also wonder whether we should not explain, in 1-2 sentences, the purpose of each of those tables + figures - i.e., what can be seen from them (DONE)
\begin{itemize}
\item Two tables showing statistics of running times for support and challenge data, respectively, to summarize the data set. An example of the support data summary is illustrated in Table~\ref{tab:ESA-support}.
\begin{table}[t]
\caption{Support data example} \label{tab:ESA-support}
\begin{centering}
\scalebox{0.75}{%
\begin{tabular}{c|cccc}
\hline
$n$ & 500 & 600 & $\cdots$ & 1500 \tabularnewline
\hline
\# instances & 1000 & 1000 & & 1000 \tabularnewline
\# running times & 1000 & 1000 &  & 1000 \tabularnewline
mean & $2.67$ & $4.154$ &  & $34.06$ \tabularnewline
coefficient of variation & $0.3943$ & $0.8558$ &  & $1.226$ \tabularnewline
Q(0.1) & $2.24$ & $3.26$ & $\cdots$ & $19.17$ \tabularnewline
Q(0.25) & $2.35$ & $3.42$ &  & $20$ \tabularnewline
median & $2.49$ & $3.61$ &  & $21.57$ \tabularnewline
Q(0.75) & $2.64$ & $3.84$ &  & $27.29$ \tabularnewline
Q(0.9) & $2.84$ & $4.18$ &  & $57$ \tabularnewline
\hline
\end{tabular}}\medskip{}
\par
\end{centering}
\end{table}

\item A table presenting fitted models and corresponding root mean squared error (RMSE) values, which can be used to easily see which model best fits the data according to the challenge RMSE (which is highlighted in boldface), as illustrated
in Table~\ref{tab:ESA-fitted-models}.
\begin{table}[t]
\caption{Fitted models example} \label{tab:ESA-fitted-models}
\begin{centering}
\scalebox{0.75}{%
\begin{tabular}{ccccc}
\hline
 &  & \multirow{2}{*}{Model} & RMSE  & RMSE\tabularnewline
 &  &  & (support)  & (challenge)\tabularnewline
\hline
\hline
\multirow{3}{*}{EAX} & Exp. Model & $1.6504\times 1.0017^{x}$ & $7.1101$ & $\left[1421.6,1678.1\right]$ \tabularnewline
 & RootExp. Model & $\mathbf{0.24807\times 1.1229^{\sqrt{x}}}$ & $\mathbf{2.298}$ & $\mathbf{\left[26.147,182.98\right]}$ \tabularnewline
 & Poly. Model & $1.9222\times10^{-5}\times x^{1.9053}$ & $0.098977$ & $\left[159.78,385.31\right]$ \tabularnewline
\hline
\end{tabular}%
}\medskip{}
\par
\end{centering}
\end{table}

\item A figure showing running times, fitted models and corresponding bootstrap confidence intervals for each model, which provides a very useful and easy to understand visualization of the analysis performed, as illustrated in Figure \ref{fig:ESA-fitted-models}.
\begin{figure*}[t]
\noindent \begin{centering}
\includegraphics[width=0.7\textwidth]{EAX_fittedModels} \vspace{-5mm}

\par\end{centering}

\caption{Example output of ESA -- running times, fitted models and corresponding bootstrap confidence intervals.}\label{fig:ESA-fitted-models}
\end{figure*}

\item A figure showing the residues of the fitted models, which helps the user easily see trends in the residues of the fitted models, as illustrated in Figure~\ref{fig:ESA-fitted-residues}.
\begin{figure*}[t]
\noindent \begin{centering}
\includegraphics[width=0.7\textwidth]{EAX_fittedResidues} \vspace{-5mm}

\par\end{centering}

\caption{Example output of ESA -- residues of the fitted models}\label{fig:ESA-fitted-residues}
\end{figure*}


\item A table of bootstrap confidence intervals for all model parameters, which allows a user to assess the uncertainty in the fitted models and perhaps accept or reject hypotheses about whether or not empirical observations match theoretical expectations about an algorithm's scaling. An example is shown in Table~\ref{tab:ESA-bootstrap-paras}.
\begin{table}[t]
\begin{centering}
\caption{\label{tab:ESA-bootstrap-paras}Model confidence intervals example}
\scalebox{0.75}{%
\begin{tabular}{cc|cc}
\hline
Solver  & Model  & Confidence interval of $a$  & Confidence interval of $b$ \tabularnewline
\hline
\multirow{3}{*}{EAX} & Exp. & $\left[1.6182,1.6777\right]$ & $\left[1.0017,1.0018\right]$ \tabularnewline
 & RootExp. & $\left[0.23777,0.2565\right]$ & $\left[1.1218,1.1244\right]$ \tabularnewline
 & Poly. & $\left[1.6402\times10^{-5},2.1752\times10^{-5}\right]$ & $\left[1.8875,1.9281\right]$ \tabularnewline
\hline
\end{tabular}%
}\medskip{}

\par\end{centering}

%\caption{\label{tab:ESA-bootstrap-paras}Example output of ESA -- bootstrap confidence intervals for all model parameters.}
\end{table}

\item A table of medians and bootstrap confidence intervals for the support and challenge RMSE of each model, which provides more information about how well the models fit the data than Table~\ref{tab:ESA-fitted-models} by leveraging the bootstrap analysis, as illustrated in Table~\ref{tab:ESA-bootstrap-RMSE}.

\begin{table}[t]
\begin{centering}
\caption{Model RMSE example}\label{tab:ESA-bootstrap-RMSE}
\scalebox{0.75}{%
\begin{tabular}{cc|cc|cc}
\hline
 \multirow{2}{*}{Solver} & \multirow{2}{*}{Model} & \multicolumn{2}{c|}{Support RMSE}  & \multicolumn{2}{c}{Challenge RMSE} \tabularnewline & & Median & Confidence Interval & Median & Confidence Interval \tabularnewline\hline
\hline
\multirow{3}{*}{EAX} & Exp. & $1.0862$ & $\left[1.0286,1.1404\right]$ & $1577.3$ & $\left[1209.6,1801.1\right]$ \tabularnewline
 & RootExp. & $\mathbf{0.61754}$ & $\mathbf{\left[0.56284,0.67104\right]}$ & $\mathbf{71.045}$ & $\mathbf{\left[12.057,394.81\right]}$ \tabularnewline
 & Poly. & $0.15889$ & $\left[0.11768,0.2076\right]$ & $251.38$ & $\left[121.37,592.91\right]$ \tabularnewline
\hline
\end{tabular}%
}\medskip{}

\par\end{centering}

%\caption{Example output of ESA -- medians and bootstrap confidence intervals for support and challenge RMSE.}

\end{table}

\item Two tables, for each model, of bootstrap confidence intervals for
observed and predicted running times, one for support data and the
other for challenge data. These tables allow the user to easily identify which model predictions are weakly or strongly consistent with the observations through boldface and asterisks. An example for challenge data is illustrated in Table~\ref{tab:ESA-bootstrap-challenge}.
\begin{table*}[t]
\begin{centering}
\caption{Observed and predicted challenge confidence intervals example}
\label{tab:ESA-bootstrap-challenge}
\scalebox{0.75}{%
\begin{tabular}{ccccc}
\hline
\multirow{2}{*}{Solver} & \multirow{2}{*}{$n$} & Predicted confidence intervals & \multicolumn{2}{c}{Observed median run-time}\tabularnewline
 &  & RootExp. model  & Point estimates  & Confidence intervals\tabularnewline
\hline
\hline
\multirow{4}{*}{EAX} & 2000 & $\left[43.72,45.07\right]$ & $41.26$ & $\left[40.05,42.41\right]$ \tabularnewline
 & 2500 & $\mathbf{\left[80.21,83.7\right]}$ & $86.62$ & $\left[62.31,119.2\right]$ \tabularnewline
 & $\cdots$ & $\cdots$ & $\cdots$ & $\cdots$ \tabularnewline
 & 4500 & $\mathbf{\left[571.2,620.6\right]}$ & $\left[368.4,982.4\right]$ & $\left[257.9,1528\right]$ \tabularnewline
\hline
\end{tabular}%
}\medskip{}

\par\end{centering}

%Example output of ESA -- bootstrap confidence intervals for observed running times and predictions from the fitted root-exponential model for the support data (upper table) and the challenge data (lower table); ``...'' represents omitted lines analogous to those shown.
\end{table*}
 
\end{itemize}
A snapshot of the report generated by ESA using the default {\LaTeX}
template is shown in Figure \ref{fig:Snapshot-ESA-output}, the full report can be found at \url{www.cs.ubc.ca/labs/beta/Projects/ESA/samples/scaling\_EAX.pdf}. 
\begin{figure*}[t]
\begin{centering}
\includegraphics[width=0.48\textwidth]{ESA_output_1}\includegraphics[width=0.48\textwidth]{ESA_output_2}
\par\end{centering}

\caption{A snapshot of the 7-page technical report generated by ESA.}\label{fig:Snapshot-ESA-output}
\end{figure*}


\section{Benchmark sets}
\label{sec:AA}

%HH: I'd make it clearer in this section intro that we proceed in two stages; first, we do "stress testing" (don't use those words), for which we need these benchmarks; then, we discuss "real-world" applications (DONE: YP)
In order to assess the quality of the results obtained by ESA, it is important to study application both ``real'' application scenarios (\ie{}, running time data sets obtained by running an algorithm on a set of instances) and ``artificial'' application scenarios, where we have complete control over the properties of the running time data set and hence we can verify that ESA produces the correct output. In computing science, it is common practice to analyse theoretical properties of algorithms, such as the worst case running time complexity; however, since in practice the observed scaling may be much better than asymptotic worst-case bounds, we cannot rely on the theoretical properties of an algorithm to justify whether or not ESA has correctly identified the true empirical scaling. To this end, we have developed a general technique for producing approximately realistic running time data sets with known scaling properties. We perform a rigorous analysis of ESA's performance on such artificial data sets and derive advice and best-practices in Section~\ref{sec:Stress Testing} and we perform additional experiments investigating ESA's performance on artificial data sets with competing, lower-order terms in Section~\ref{sec:Lower Order Terms}. Finally, in Section~\ref{sec:Successful Applications}, we present a summary of successful applications of ESA's methodology on ``real-world'' data sets. 

Any artificially generated data set of running times should display to the greatest possible degree characteristics similar to those of realistic application scenarios. 
Towards this end, we simulated a randomized algorithm with three distinct sources of variability in running time: 
variance due to differences between problem instances (of the same size); variance in running times between multiple independent runs on the same problem instance; and changes in running times as a function of instance size.
To simulate the variance from independent runs on a single instance, we draw samples from an exponential distribution, parameterized to have a median running time equal to the desired running time for the instance. 
We chose to model the variance with an exponential distribution to closely resemble behaviour observed for a range of prominent stochastic local search algorithms for SAT~\cite{HooStu99}. 
Similarly, to determine the median running time for a given instance, we draw a sample from an exponential distribution with a prescribed median. While we expect that the distribution in running times between instances will vary between applications, we chose an exponential distribution because we have observed high variability and heavy tails in (median) running times across instances of the same size for several scenarios we studied previously (\ie{}, three complete and three incomplete SAT solvers for Random 3-SAT phase transition instances~\cite{MuHoo15} and two state-of-the-art, inexact TSP solvers on RUE instances~\cite{MuEtAl16}).
%HH: Can we justify this assumotion? I'd refer at least to observed high variability in (median) running times across insances of the same size ... (DONE YP)
Finally, to determine the median running time for a given instance size, we use a given scaling model mapping instance size to median running time.

As an example, assume we want to generate running times for a randomized algorithm with quadratic scaling on an instance of size $n = 1\,000$. 
First, we would pick the running time scaling model, e.g., $10^{-6} \cdot n^2$, and use it to compute the median running time for instance size $1\,000$ -- in this case $10^{-6} \cdot 1\,000^2 = 1$ (CPU second). 
Second, we draw a sample from an exponential distribution with median $1$. 
In this case, assume we draw a value of $0.83291$ (CPU seconds); this means that $0.83291$ is the median running time for that particular instance. 
Finally, if we want to simulate 3 independent runs of the algorithm on this instance, we would draw 3 samples from an exponential distribution with median $0.83291$.


\section{Stress testing}
\label{sec:Stress Testing}

There are many potential factors that could cause ESA to report misleading or incorrect results: for example, it could erroneously accept an incorrect scaling model because of misleading lower-order terms, or because the predicted confidence intervals are so large that any model fits the data. 
Clearly, the latter case is more benign than the first, but it is not always clear how to resolve the problem. 
%HH: new text:
To better understand the robustness and limitations of ESA, we conducted a series of carefully designed \emph{stress-testing experiments}.
Specifically, by varying properties of artificially generated benchmark sets of running time (see Section~\ref{sec:AA}), we studied the performance of ESA for a range of challenging situations, \ie{},
\begin{itemize}
    \item decreasing the number of instances per instance size;
    \item decreasing the number of support instance sizes;
    \item reducing the number of independent runs per instance;
    \item increasing the extrapolation distance; and
    \item decreasing the number of bootstrap samples ESA uses.
\end{itemize}
We discuss the additional challenges imposed by the presence of competing, lower order terms in an algorithms running time scaling in Section~\ref{sec:Lower Order Terms}.

%HH: You need to explain somewhere (perhaps here), which kind of stress tests you describe in this section, and which in the next. At the very least, because you mention lower-order terms above, you should make it clear that you are investigating the degree to which they cause issues in Section 6. (DONE)

We generated two data sets, using a polynomial and an exponential scaling model. For the polynomial model, we used $2.58\cdot 10^{-10} \cdot n^{3.37}$, which tended to fit some real running time data obtained from a TSP solver in preliminary experiments. 
We fitted the exponential model to data from the polynomial model, to make the two models as similar as possible. 
We generated running times for 21 instance sizes: 500, 600, ..., 1900, 2000, 2500, ..., 4500, and we used 16 support instance sizes, 5 challenge instance sizes, 500 instances per size and 10 independent runs per instance. 
%HH: I am confused - does the following speak of the same instance set? I think not ... yet, no clear distinction is drawn ...
%YP: I've re-organized the text a bit. Is it more clear now? (I am talking about the same two sets of instances -- one for the polynomial model and one from the exponential model)
Since the instance sets are sampled from a probability distribution, we created a very large data set with 10\,000 instances per size and 100 independent runs per instance. 
This allowed us to perform 1\,000 independent runs of ESA on various sub samples of the original data sets with the desired properties, \eg{}, 100 instances per support instance size.
For all of our experiments, we set ESA's parameters to their default values, using 1\,000 bootstrap samples (unless otherwise indicated) and an alpha value of 95, and we studied the median running time of the per-instance medians. 

In the following, we provide only a high-level summary of our findings, and distill from these results generic advice on best-practices for using ESA. 
For a substantially more detailed discussion and presentation of the results, please see our online, supplementary material available at \url{http://www.cs.ubc.ca/labs/beta/Projects/ESA}~\todo{Not yet online (though ESA v1.1 is). Currently available at https://www.overleaf.com/6216557917nztjdzchjhxp}
%HH: <- TODO: put on-line before submission

%HH: All of the following should be a bit more concrete, without being significantly longer. Rather than giving vague statements, such as "say 10", refer to concrete examples of what you have seen. If we don't do this, giving details on the experimental setup (above) is useless, and we run a risk that reviewers simply aren't convinced. (DONE)

\textbf{What happens when we decrease the number of instances per instance size?}
We studied ESA's performance with 10, 20, 50, 100, 200, 500 and 1\,000 instances per instance size.
We found that ESA can identify that the correct model fits the data even with a very small number of instances per size; however, this is mostly because the size of the bootstrap confidence intervals for the fitted model predictions grows much more quickly than the size of the confidence intervals for the observed challenge statistics, \ie{}, all of the fitted models fit the data very well for small instance sets. 
For example, for the polynomial model on the polynomial data set and for 10 support instances per size, the median confidence interval size for predictions on challenge instance size 4\,500 was 247.2, where we measure the size of a confidence interval as the upper bound divided by the lower bound, \ie{}, the upper bound of the 95\% confidence interval was 247.2 times larger than the lower bound, for the median size of the confidence intervals determined  from our 1\,000 runs of ESA. 
On the other hand, the median confidence interval size for observed running times on instance size 4\,500 was only 6.1, \ie{}, the interval for performance predictions was 40.2 times larger than that for observations.
In comparison, when using 1\,000 instances per instance size, the interval for predictions was only 1.4 times larger than that for observations. 
We observed qualitatively similar results for the other models and on the exponential data set. 
Given how much more quickly the confidence intervals for predictions grow than those for observed performance as we decrease the number of instances per instance size, it is unsurprising that ESA determines that all of the fitted models fit the challenge data very well.

\textbf{What happens when we decrease the number of support instance sizes?}
To avoid conflating changes to the number of support instance sizes with changes to other properties of the data set (\eg{}, the extrapolation distance or the range covered by the support sizes), we fixed the location of all of the challenge instance sizes and the largest support instance size, and we forced the smallest support instance size to always be either 500 or 600. 
Then, to control the number of support instance sizes we varied their ``density''. 
For example, when using 8 support instance sizes instead of 16, we used every second support instance size from our overall settings of $n=500, 600, ..., 2000$.

To our surprise, we observed that ESA reported far less false positives in this case than when we reduced the number of instances per instance size, relative to the total number of support instances available. 
For example, the square-root exponential model was reported to tend to fit the data 6.4\% of the time for the polynomial data set and 14.7\% of the time for the exponential data set when given only 3 support instance sizes, compared to 51.4\% and 51.0\% of the time, for the polynomial and exponential data sets respectively, when given 16 support instance sizes, but only 100 instances per size -- a roughly comparable total number of support instances.
Specifically, when given 1600 instances spread between 16 support instance sizes, ESA reported false positives 45.0\% less often for the polynomial data set and 36.3\% less often for the exponential data set, than when given 1500 instances spread between 3 instance sizes. 

However, while this may indicate a good option for saving on computational expenses, we advise extreme caution when analyzing results with ESA that use very small numbers of support instance sizes. 
In particular, when ESA does report a false positive, it does so with predicted confidence intervals that are orders of magnitude smaller than in the case with less instances per instance size; \eg{}, for instance size 4\,500 the median predicted interval size for the square-root exponential model on the polynomial data set was 3.2 when using 1500 instances spread between 3 instance sizes, compared to 7.4 for 1600 instances spread between 16 support instance sizes. 
As a result, a user may be lead to incorrectly assume that ESA's best-fit model accurately captures the true scaling. 
In real scenarios, we expect there to be an added challenge for ESA: coping with the effects of lower order scaling terms, which would likely significantly increase the probability that ESA will incorrectly classify the scaling when only a few support instances sizes are used. 
Furthermore, as we discuss in Section~\ref{sec:Lower Order Terms}, the best safeguard of which we are aware against making incorrect assumptions due to lower order terms consists of looking at the degree to which the model fits both the support and challenge data. When a very small number of support instance sizes are available, this type of safeguard becomes unavailable, because there are not enough small support instance sizes to determine whether or not there is a systematic deviation pulling the fitted model away from the running times observed on the smallest support instance sizes.
%HH: <- why? please clarify. (DONE)

\textbf{What happens when we reduce the number of independent runs per instance?}
%HH: shouldn't we state clearly that multiple independent runs per instance are used to obtain stable performance estimates for randomised algorithms as well as in situations where there is significant noise in the execution environment? (DONE)
For randomized target algorithms, or in situations where significant noise in the execution environment is present, it is common to perform multiple independent runs of the algorithm on each instance and then take the median running time for each instances in order to obtain stable performance estimates~\cite{Sun09}.
We studied a set of 6 values for the number of independent runs per instance: 1, 2, 5, 10, 20 and 50. 

We observed a similar trend as when we reduced the number of instances per instance size; however, the decrease in confidence interval size and the increase in false positives are more benign in this case. 
In particular, consider the decrease from 10 runs per instance to 1 run per instance, compared to the difference from using 500 instances per instance size to 50 instances per size. 
In both cases, we are decreasing the total number of algorithm runs by a factor of 10. 
However, the response in the size of the bootstrap intervals, and hence in ESA's interpretation of the model fit, is drastically different in the two cases. 
In particular, for 1 run per instance, the median size of the bootstrap interval for the running time predicted by the polynomial model on the polynomial data set for instance size 4\,500 is 2.6, compared to 9.9 for 50 instances per instance size. 
The percentage of false positives for the square-root exponential model on the polynomial data set is only 4.1\% with 1 run per instance compared to 74.1\% with 50 instances per instance size -- \ie{}, there are 18.1 times as many false positives for the square-root exponential model on the polynomial data set when reducing the number of instances per size by a factor of 10 than when reducing the number of runs per instance by a factor of 10. 
%HH: <- better to express this as a factor in the false positive rate (DONE: change)

Based on this striking difference, it is clear that if the time required to collect all of the running time data is constrained, then the best option is to use very few independent runs per instance and more instances per instance size, assuming these instances are available. This result aligns with our expectations, since using multiple instances captures variability due to both randomness in the algorithm (and/or noise in the execution environment) as well as differences between instances, whereas performing multiple runs per instance captures a strict subset of the total variability. This insight also underlies a theoretical result by Birattari~\cite{Bir04}, which shows that using only a single run per instance with many instances is the optimal choice when estimating the mean performance of a randomised algorithm over a distribution of multiple problem instances. 
%HH: <- can we speculate why that is? it seems that using multiple instances captures variability due to randomness in the algorithm (and/or noise in the execution environment) as well differences between instances of the same size ... this is similar to the result by Birattari for estimating average running time for randomised algorithms on sets of problem instances (which we should likely cite here; DONE)

\textbf{What happens when we increase the extrapolation distance?}
To isolate the effect of varying the extrapolation distance, we used only a single challenge instance size for these experiments. 
We also used only 11 support instance sizes, 500, 600, ... 2\,000, instead of the 16 used in the previous experiments, in order to work with 5 different challenge instance sizes: 2\,500, 3\,000, ..., 4\,500.

Overall, these results line up well with our intuition: the farther the extrapolation, the higher the probability that ESA will correctly identify the true scaling and reject incorrect scaling hypotheses. 
Consider, for example, the exponential data set. The exponential model was reported to tend to fit or to fit the data very well in at least 99.8\% of runs for each location of the challenge instance size. 
However, as the challenge instance size was moved from 2\,500 to 4\,500, the percentage of times it was reported to fit the data very well increased from 16.7\% to 88.1\%. 
At the same time, the square-root exponential model was reported to tend to fit the data 94.8\% of the time with challenge size 2\,500, but only 0.1\% of the time with challenge size 4\,500. 
We obtained analogous results for the polynomial data set. 
While this may seem somewhat unsurprising, it does indicate that the separation of the models fitted by ESA grows more quickly than the size of the predicted intervals, otherwise ESA's ability to distinguish between the models would not increase. 
As a result, increasing the extrapolation distance is one of the best ways to obtain more reliable and statistically significant results with ESA. 
Of course, this comes at the cost of the target algorithm runs themselves requiring more running time. 
%HH: <- let's make sure that we introduce the term 'target algorithm' for the algorithm whose performance we are investigating using ESA early in the paper, and then use it in places like this, where there may be ambiguity between the target algorithm and ESA itself. (DONE)

\textbf{What happens when we decrease the number of bootstrap samples used by ESA?}
We tried seven values for the number of bootstrap samples used by ESA with approximately logarithmic spacing: 20, 50, 100, 200, 500, 1\,000 and 2\,000. We found that modifying this parameter had a mostly negligible effect on ESA's performance, which is somewhat surprising, especially in the context of very small numbers of bootstrap samples. Overall, the largest effect that we observed was a change in ESA's running time, which is roughly linear with the number of bootstrap samples. 
We believe that we would have observed more significant effects on ESA's performance had we also used less support data, so we still recommend to use at least 1\,000 bootstrap samples, since the cost is typically small relative to performing additional algorithm runs. 
On the other hand, we observed no significant benefit to increasing the number of bootstrap samples beyond 1\,000.


\section{Lower-order terms}
\label{sec:Lower Order Terms}

One possible source of difficulty for ESA occurs when a given target algorithm shows scaling of running time characterised by a function that includes lower-order terms in addition to the asymptotically dominant term.
For example, an algorithm may show exponential asymptotic scaling of running time with instance size; however, for small instance sizes, the scaling may appear to be polynomial, because the running times are dominated by polynomial costs incurred by initializing data structures. 

To investigate these effects, we used running time data sets generated with two polynomial terms with degrees 2 and 5. For the degree-2 polynomial term, we used the coefficient $9.6\cdot 10^{-7}$, and for the degree-5 polynomial term, we considered three coefficients: $4.8\cdot 10^{-15}$, $4.8\cdot 10^{-16}$ and $4.8 \cdot 10^{-17}$. 
These values were chosen so that the transition occurs near the low, middle and high end of our support instance sizes, respectively. 
Our data sets contained 11 independent runs per instance with 1\,000 instances for each of the 21 instance sizes 500, 600, ..., 1\,900, 2\,000, 2\,500, ..., 4\,500. 

In addition, we ran ESA three times using three different values for the number of support instance sizes: 8, 12 and 16; in each case, all remaining instance sizes were used as challenge data. 
These experiments produced a total of nine different ESA reports, which we examined in detail. 
We also provided ESA with a four-parameter, two-term polynomial model of the form $a\cdot n^b + c\cdot n^d$, to determine if the scaling behaviour could be correctly identified, and accurate scaling models could be produced.

\textbf{What happens when the transition is early?}
\begin{figure*}[t]
\centering
\includegraphics[width=0.7\textwidth]{fittedModels-2-5-14-16s.pdf}
\caption{\textbf{Early transition example:} generated using $4.8\cdot 10^{-15} \cdot x^5 + 9.6\cdot 10^{-7} \cdot x^2$ with 16 support instance sizes. \note{We'll need to decide whether or not we want colour figures. They cost extra, but you have to contact them to obtain a quote for the price. I think for these three figures in particular it would be nearly impossible to understand them without colour (and I'm not confident that I could make them legible without). HH: We can decide this once the paper has been accepted. Leave as colour figures for now.}}
\label{fig:AA-competing-2-5-14-16s}
\end{figure*}
When the transition between dominant terms occurs within the lower range of small support instance sizes, the fit of the single-term model is able to capture the asymptotic scaling relatively accurately, \eg{}, see Figure~\ref{fig:AA-competing-2-5-14-16s}, where a polynomial model of degree 4.97 fits the challenge data very well. In comparison, the two-term polynomial model provides a better fit for the small instance sizes, but yields larger predicted bootstrap intervals. For this model, ESA fit a degree of 2.00 for one of the polynomial terms and a degree of 5.14 for the other, and reported that this model also fit the data very well. 
%HH: added
Considering the large number of parameters in the more flexible two-term polynomial model, this is not surprising.

These results are positive, but we note that ESA experiences some difficulties fitting the 4 parameter model. 
In particular, higher-quality initial parameter values are needed for the two term model than for single-term models. Furthermore, the confidence intervals for the degrees of each term in the four-parameter model are relatively large and overlapping, at $b \in [1.41,4.87]$ and $d \in [4.77,6.26]$. 
We believe that this is caused by outliers in the data for large support instances sizes that lead to some model over-fitting within ESA.

\textbf{What happens when the transition occurs near the middle of the support range?}
\begin{figure*}[t]
\centering
\includegraphics[width=0.7\textwidth]{fittedModels-2-5-15-16s.pdf}
\caption{\textbf{Mid transition example:} generated using $4.8\cdot 10^{-16} \cdot x^5 + 9.6\cdot 10^{-7} \cdot x^2$ with 16 support instance sizes.}
\label{fig:AA-competing-2-5-15-16s}
\end{figure*}
As the transition between the two terms moves closer to the large end of the support instance sizes, the quality of the ESA report starts to degrade. Overall, ESA is still able to do quite well, as long as the location of the transition is completely covered by the support instance sizes, as seen in Figure~\ref{fig:AA-competing-2-5-15-16s}, where the bootstrap intervals for the predictions obtained from both types of polynomial models capture the observed challenge data. 
The single-term model is reported to ``fit the data very well'', although it does not quite capture the true degree of the asymptotic scaling with a reported confidence interval of $[4.00,4.74]$. 
On the other hand, the two-term model, which is only reported to ``tend to fit the data'', does capture the true asymptotic scaling, with the confidence intervals of $b \in [1.50,2.30]$ and $d \in [4.78,7.06]$. 

In this case, we also see that ESA had less trouble distinguishing between the two polynomial terms when fitting the two-term polynomial model, as evident from the disjoint bootstrap intervals for $b$ and $d$.
However, we also see that the size of the predicted bootstrap intervals for the two-term model has increased significantly (see Figure~\ref{fig:AA-competing-2-5-15-16s}). 
We believe this is causes by the fact that there is only a small number of instance sizes past the midpoint of the transition, and hence less data to help ESA recover from the effects of outliers. 
When we decrease the number of support instance sizes (data not shown) we find that the predicted bootstrap intervals are similar in size for the two-term polynomial model; however, the single-term model is unable to accurately capture the asymptotic scaling.

\textbf{What happens when the transition is late?}
\begin{figure*}[t]
\centering
\includegraphics[width=0.7\textwidth]{fittedModels-2-5-16-16s.pdf}
\caption{\textbf{Late transition example:} generated using $4.8\cdot 10^{-17} \cdot x^5 + 9.6\cdot 10^{-7} \cdot x^2$ with 16 support instance sizes.}
\label{fig:AA-competing-2-5-16-16s}
\end{figure*}
The worst-case scenario for ESA occurs when the transition in dominance between two competing terms occurs for instance sizes close to or beyond the largest support instance size, so that the true asymptotic scaling is heavily obscured on the given running time data. 
This can be seen in Figure~\ref{fig:AA-competing-2-5-16-16s}, where the square-root exponential model appears to fit the data very well. 
However, ESA also identifies that the two-term polynomial model fits the data very well, and thereby does not dismiss the correct scaling model.
In practice, the safest course of action in such cases is to collect more running time data -- in particular, for larger challenge instance sizes -- and to run ESA again. 
In cases where this is impossible, a pragmatic user would be inclined to choose the square-root exponential model as the one that is the best fit, while keeping in mind that it may be an over-estimate for the true running time scaling. 
In our example, we can see limited support that the square-root exponential model is an over-estimate by examining the smallest support instance sizes, for which the curvature of the square-root exponential model is just beginning to pull the model above the observed running times.
%HH: <- also point to residual plots? It might even be worth including one ... (DONE)
%YP: I chose not to include one here. Looking at the residual plot, it's not a great example, as the deviations are relatively minor (although they are larger for the sqrt-exp model than the two terms poly model). I've updated the wording a bit to indicate this. Indeed, I chose this example because the sqrt-exp model appears to be a very good fit for the data. 
Similar situations may occur in other scenarios where the best-fit model may not accurately capture the asymptotic scaling due to the effects of lower-order terms. One useful method for detecting this is to examine the residual plots generated by ESA, and in particular to zoom in on the smallest support instance sizes to look for systematic deviations or trends indicating that the model may not accurately predict the data for smaller instance sizes (and hence may also not fit the data for larger instance sizes).

We also observed that the size of the bootstrap intervals for predicted running times continues to increase for the four-parameter model. This is even more clear for the case with only 12 support instance sizes (data not shown), where the bootstrap intervals for predicted running times are very large ($4.4\cdot 10^4$ for instance size 4\,500). 
When running ESA on a variant of this scenario with only 8 support instance sizes, we found that the Levenberg-Marquardt algorithm simply was unable to fit the four-parameter model to the data -- even when the default fitting parameters were set to the true values for the running time scaling, the implementation of the Levenberg-Mardquart algorithm used in ESA simply crashed. 

\textbf{What happens if we have prior knowledge about the lower-order terms?}
Practical applications of ESA to algorithms with competing terms may have known scaling for start-up costs; \eg{}, the initialization of a data structure may be known to have quadratic scaling. 
Hence we may be inclined to use a scaling model that captures this prior knowledge, such as a three-parameter, two term polynomial model of the form $a\cdot n^b + c\cdot n^2$. 

We ran ESA again on each of our 9 scenarios; however, this time we used the three-parameter, two-term polynomial model $a\cdot n^b + c\cdot n^2$. 
Overall, the results produced by ESA did not change much. 
In a few cases, the fit of the 3-parameter model was slightly better for the small instance sizes; however, it appeared to remain unchanged for the challenge instance sizes. 
We also observed that the bootstrap intervals for predicted running times were slightly smaller and located slightly higher in most of the scenarios. 
The only exception to this observation was for the function $4.8\cdot 10^{-15} \cdot x^5 + 9.6\cdot 10^{-7} \cdot x^2$ when ESA was run with 8 support instance sizes  (a mid-range transition scenario).
We found that the bootstrap confidence intervals for predictions obtained from the four-parameter model were very large (comparable to those in Figure~\ref{fig:AA-competing-2-5-16-16s}); however, the three-parameter model produced substantially smaller confidence intervals (comparable to those in Figure~\ref{fig:AA-competing-2-5-15-16s}). 
Unfortunately, the Levenberg-Mardquart algorithm was still unable to run successfully on some of the 1000 bootstrap samples of the $4.8\cdot 10^{-17} \cdot x^5+ 9.6\cdot 10^{-7} \cdot x^2$ data set when only 8 support instance sizes were used. 
%HH: <- reworded this; make sure it is still true (DONE)

\section{Successful applications}
\label{sec:Successful Applications}

During the development of ESA, earlier versions were used in several projects to analyze the empirical scaling of high-performance algorithms for several widely studied NP-hard problems. 
These early applications provided interesting results, as well as valuable insights that guided the development of the version of ESA presented here.
%HH: I added the following, but this doesn't quite set the stage for what follows. Please modify / amend this to make it clear what we summarise in the following and why (e.g., why don't we outline the first, published applications?) - DONE
%YP: I'm not too sure exactly why that section is there (as it was there before I started working on the paper). My best guess is that at the time all of those examples were unpublished anywhere other than Zongxu's thesis (although this isn't true now).
%HH: I've left some detailed notes below. The main idea is to briefly summarise all results obtained with previous versions of ESA
In the following, we outline the findings obtained from these earlier applications.

%HH: new:
The methodology underlying ESA was first used to study the empirical scaling of the running time of Concorde, a prominent TSP solver.
Concorde represents the long-standing state of the art in exact TSP solving; it incorporates mechanisms based on over 50 years of research on the TSP and has been used to solve the largest non-trivial TSP instances for which provably optimal solutions are known~\cite{AppEtAl06,AppEtAl12}.
%HH: refs can be found in HooStu14
%HH: now briefly outline the main findings of HooStu14 (DONE)
We fitted an exponential model of the form $a\cdot b^n$ with $b \approx 1.003$ to the data; however, it was rejected with 95\% confidence in favour of a square-root exponential model of the form $a\cdot b^{\sqrt{n}}$, with $b \approx 1.24$. 

%HH: reword the following to first outline DubEtAl15, then talk about additional results. I doubt we want to show any results for earlier versions of ESA, so take out all graphs etc. that are not produced by the latest version (DONE)
In another application of ESA, we studied the scaling for the state-of-the-art inexact TSP solvers, EAX~\cite{NagKob13} and LKH~\cite{helsgaun2000effective} on the same set of random uniform Euclidean instances~\cite{DubEtAl15}. However, since EAX and LKH are inexact solvers, they cannot prove the optimality of the solutions they find, and hence we recorded the running times required to find the optimal solution qualities previously found using Concorde. Unfortunately, some of the larger problem instances remained unsolved even with long runs of Concorde, which added additional uncertainty into the scaling analysis.

We found that EAX likely scaled as a square-root exponential model, with some possibility of polynomial scaling. We studied two versions of LKH (1.3 and 2) and we found that version 1.3 appeared to scale as a square-root exponential model, while version 2 appeared to have observed scaling between a square-root exponential model and a polynomial model. All three methods obtained better scaling for finding optimal solution costs than Concorde, with degrees $b \approx 1.12, 1.20 \text{ and } 1.18$ for the square-root exponential models fitted for EAX, LKH 1.3 and LKH 2, respectively. Later, we improved these results with even longer runs of Concorde to find the optimal solutions for nearly all of the instances (see Section 6.3 in~\cite{Mu15}). ESA confirmed that EAX scaling is consistent with a square-root exponential model, but for LKH 2, we continued to observe scaling between the two models. 

Finally, using ESA, we investigated the impact of parameter settings and automated algorithm configuration on the performance scaling of the two inexact TSP algorithms~\cite{MuEtAl16}. For EAX, algorithm configuration helps improve the scaling, which can be further improved by adapting the population size with instance size. In particular, we achieved an $\approx1.13$-fold improvement in the median running time for EAX to solve RUE instances of size $n=4\,500$ and expect the improvement to be even more significant for larger instances. For LKH, we observed significant impact of parameter settings on performance scaling, but the state-of-the-art algorithm configurator SMAC~\cite{hutter2011sequential} tends to overfit the running times for smaller instances and thus produces configurations for which LKH scale worse.

In a second line of work, the empirical scaling analysis approach underlying ESA has been used to study high-performance solvers for the propositional satisfiability problem (SAT).
Specifically, we studied three prominent SLS-based solvers, WalkSAT/SKC~\cite{selman1994noise}, BalancedZ~\cite{li2014balance} and probSAT~\cite{balint2014probsat}, and three prominent DPLL-based solvers, kcnfs~\cite{dequen2004kcnfs}, march\_hi~\cite{heule2009march} and march\_br~\cite{heule2013march} (version SAT+UNSAT), on random phase-transition 3-SAT instances~\cite{MuHoo15}. For each algorithm, we ran ESA using a polynomial model ($a\cdot n^b$) and an exponential model ($a\cdot b^n$). For the SLS-based algorithms, we looked at the median scaling on the satisfiable instances and we found that all three of the SLS-based solvers were best described by a polynomial model with degree $b \approx 3$, whereas the exponential model was inconsistent with most of the observations for the challenge data. Looking at the confidence intervals for the polynomial model parameters for the three SLS-based algorithms, we saw no evidence that any algorithm scaled significantly better than any other. In contrast, we studied the DPLL-based algorithms on both the satisfiable and unsatisfiable instances and found the opposite: they were all best described by the exponential model with $b \approx 1.03$ and were inconsistent with the polynomial model -- even when looking only at the satisfiable instances. 
%HH: <- briefly outline main findings of MoHoo15 (DONE); focus on differences between complete and incomplete solvers; mention names of solvers and give references for them (from MuHoo15). Reword the following to build on these results (DONE):

We later expanded this analysis to two classes of random 4-SAT instances. This work, described in detail in Section~5.4 of \cite{Mu15}, yielded several interesting results. For random 4-SAT phase-transition instances, an exponential model with $b \approx  1.02$ best fit the scaling for BalancedZ, whereas we needed to add a root-exponential model (which was fitted with $b \approx 2.8$ for WalksSAT/SKC and $b \approx  1.6$ for probSAT) to accurately describe the scaling for the other two SLS-based, incomplete SAT solvers. The DPLL-based, complete solvers continued to demonstrate scaling behaviour well characterized by exponential models; however, the degree of the model ($b \approx 1.1$) did increase for these models compared to that for 3-SAT instances. We also studied WalkSAT/SKC and kcnfs on a class of less-constrained random 4-SAT instances believed to be intrinsically challenging. We showed that both scale significantly better than on phase-transition random instances, in that a polynomial model best describes the observed performance scaling.

%HH: modified:
Of course, one may wonder how all these results compare to those produced by the improved version of ESA presented in this work.
The biggest change to the methodology underlying ESA is the addition of the nested bootstrap sampling procedure to handle multiple independent runs of a randomized algorithm on a given problem instance. 
Unfortunately, the only remaining copies of the data from earlier studies contain per-instance median running times, so we were unable to perform direct comparisons of the earlier and most recent versions of ESA on the original data. We therefore reran EAX on the same TSP RUE instance set (using the improved optimality results from Mu~\cite{Mu15}), but this time we performed 11 independent runs per instance.
On this data, the version of ESA described here yielded results that are qualitatively very similar to those reported in Mu~\etal{}.~\cite{MuEtAl16}. We observed that sizes of the bootstrap confidence intervals for the observed running times on challenge instances increased by 4.2\% on average, where the size of the interval is defined as its upper limited divided by its lower bound. Similarly, the size of the confidence intervals for the square-root exponential model that best describes the running times increased by an 0.1\% on average. 
This is unsurprising -- we expect an increase in confidence interval size because the resampling over multiple independent runs per instance allows us to capture additional variability in the scaling models due to randomisation of the target algorithm, whereas previously, the statistical nature of the observed median running times was not taken into account, and the relatively small size of the increase is consistent with our observations in Section~\ref{sec:Stress Testing}.
The only other change we made to ESA's methodology was to include for the first time the decision model described in Section~\ref{sec:auto-interpretation} used to automatically summarize the scaling results. However, while this analysis augments the previous scaling analysis results, it does not change them.
%HH: <- is this true? please check carefully against section 2 and modify here as needed. I am specifically worried about the modified interpretation procedure (DONE)
%YP: As far as I am aware (considering that I did not perform the previous analysis myself) it is true that these are the only two changes. I didn't mention this one previously because as far as I have seen in all previous work, there has never been any explicit reference to the statements ESA made using zongxu's previous decision model. Similar statements were made, but they were always supported with additional data, rather than being cited as ESA's summary.
%(note that there were significantly more instances available for each support instance size than for the  challenge instance sizes, hence the uncertainty of the fitted models is much smaller than the uncertainty of the challenge instance sizes). 
%By properly handling the uncertainty from independent runs of a randomized target algorithm ESA is less likely to obtain false-negative results by rejecting the correct scaling model class; however, in our experiments we have not yet observed a scenario where using the old version of ESA rejected a scaling model when the current one fails to do so.


\section{Conclusions and future work}
\label{sec:Conclusion}

In this work, we introduced the empirical scaling analyzer (ESA), an automated tool for analyzing the empirical scaling of algorithm performance with input size. 
ESA can fit multiple models on running time data collected for a given algorithm across a series of inputs of varying size and generate results in the form of technical reports. 
These reports contain easy-to-read figures and tables, as well as automatically generated interpretations. 
We also presented new methodology to appropriately handle the variance between independent runs of a randomized algorithm and a novel method for automatically interpreting the scaling analysis results.  

We presented a rigorous analysis of ESA's performance on several types of challenging scenarios. 
In many cases, if ESA's output appears unreliable (\eg{}, the size of predicted bootstrap intervals is large for all of the fitted models), more data is needed -- perhaps running times for more instance sizes, larger challenge instance sizes, more instances per size or more independent runs per instance. 
In particular, we found that increasing the number of instances per instance size is a more cost-effective means of increasing the power of the statistical analyses performed by ESA than increasing the number of independent runs per instance. 
In addition, increasing the extrapolation distance by using larger challenge instance sizes is one of the most reliable (albeit costly) means to increase ESA's ability to discriminate between different scaling models. 
%HH: <- reworded, make sure you agree (DONE)
Based on our extensive empirical analysis, we also caution against using small numbers of support instance sizes, since this can make it challenging or impossible to identify detrimental effects of lower-order terms or outliers on the results obtained from ESA.
From our experience, we recommend to use around 11 support instance sizes, although the exact number required varies between application scenarios. 

We found that ESA can correctly recognize the asymptotic performance scaling of a given algorithm when lower-order terms are present, provided that the transition between the two competing terms of the scaling model occurs towards the lower end of the support instance sizes used for the scaling analysis. 
However, increasing the number of parameters in a parametric model to capture the lower-order terms and the asymptotic scaling substantially increases the size of the predicted bootstrap intervals and can cause ESA to experience difficulties in fitting the models. 
If the effect of a lower-order term dominates that of the asymptotic scaling behaviour across all support instance sizes, ESA is likely to correctly recognize the true asymptotic scaling.

Overall, we have found that ESA is able to perform well in most scenarios. 
Unlike theoretical running time analysis, there is always the risk that a lower-order term is initially dominating the running time, and hence larger instance sizes are needed to accurately identify the true asymptotic scaling. Nevertheless, empirical scaling analysis plays a key role in characterizing and understanding the behaviour of high-performance algorithms for important problems. 
This is particularly true for scenarios where the observed performance of algorithms exceeds the expectations provided by a worst-case analysis, as well as in cases where theoretical assumptions about the expected behaviour of an algorithm may not hold for real-world instances. 
The methodology underlying ESA is widely applicable to problems and algorithms where running time data can be gathered for various instance sizes. 
ESA provides an easy and convenient way to apply empirical scaling analysis to algorithms of interest. 
Thus, we believe that ESA will prove to be a useful tool for researchers studying both the empirical and theoretical scaling behaviour of algorithms, and we hope that ESA will promote and enhance such studies.

There are several directions for future improvements of ESA. 
In particular, it would be interesting to automatically select models from a large family of functions based on input data. 
This could also facilitate fitting of models with lower-order terms. 
One possible approach towards this end involves repeated fitting of models, first on the original data, then on the residues, in order to obtain a model with several terms. %Extended in this way, ESA could become easily applicable to an even broader range of algorithms with little human input, and may produce even more interesting results. Future work could also extend ESA to perform sensitivity analysis on each of the support instance sizes. In this way, users could be alerted if it appears that the presence of a single outlying data point is causing the fitted models to incorrectly capture the running time scaling. 
Currently, one of ESA's biggest limiting factors is the requirement that instances be grouped by size. 
Since it is not always possible to collect instances grouped by size, it would be extremely useful for many practical application scenarios to develop new methodology for empirical scaling analysis that overcomes this limitation.
Furthermore, we believe that many users of ESA may be motivated to find upper or lower bounds on the running times required to solve very large instances. 
To this end, future extensions of ESA could be developed that fit tight bounds on the running time scaling to provide users with such estimates.

Another interesting avenue of study is the performance of ESA when used to analyze the scaling of polynomial-time algorithms. 
Preliminary results indicate that such algorithms tend to show significantly less statistical variation in their running times than the heuristic, $\mathcal{NP}$-hard algorithms that have been the primary subject of our study so far. 
Very small bootstrap intervals can provide new challenges for ESA that will need to be overcome in future extensions, since in such cases, all of the fitted models tend to be rejected. 
The introduction of tight upper and lower bounds into the methodology underlying ESA may provide a way to overcome this challenge.

In addition, we are currently working on uses of ESA in the development of automated algorithm configuration procedures for better scaling behaviour. 
Such procedures could make automated configuration even more applicable to real-world situations, as problem instances of practical interest can take a long time to solve. 
Automated configuration usually requires many runs of the given target algorithm with different parameter settings, which can make it infeasible to run a configuration procedure directly on large, challenging instances.
Previous work on the problem of automatically configuring algorithms for improved performance scaling has focused on generic protocols for using existing configurators~\cite{StyEtAl12,StyHoo13}. 
An alternative consists of incorporating empirical scaling analysis, as performed by ESA, more directly into algorithm configuration. 
Unfortunately, the current version of ESA requires running time data for many problem instances of different sizes,
which can take a long time to produce. 
Thus, it will be important to design a way to reduce the time, possibly by leveraging previously fitted models, \eg{}, by integrating Bayesian methods into empirical scaling analysis, with a previous model acting as the prior for model fitting. 
This could lead to an enhanced version of ESA that could then be integrated into a future configuration procedure.


\begin{acks}
YP was supported by an NSERC Vanier Scholarship. HH acknowledges funding through an NSERC Discovery Grant, CFI JLEF funding and startup funding from Universiteit Leiden.
\end{acks}

%\bigskip
% TODO: will be deleted and changed to Acknowledgements in the final version


%\noindent
%\footnotesize
%\textbf{Note to reviewers:} 
%Two of the three case-studies mentioned
%are unpublished and only appear in the M.Sc. thesis of Mu. \note{Do we actually even need this at all? I think there are enough contributions to this paper that the ``successful applications of ESA'' could almost be moved into a section call ``related work'', that combines it with some of the material from the introduction. HH: IMO, successful applications are very important, because otherwise, we only talk about results on artificial data, showing limitations. But the note to reviewers is not needed if these results are correctly framed (with pointers to the thesis) in Section 7. YP: Makes sense, I think it does improve the paper the way it is done now.}




%\begin{figure}[t]
%\includegraphics{}
%\caption{Figure caption.}\label{f1}
%\end{figure}

%\begin{table*}
%\caption{} \label{t1}
%\begin{tabular}{lll}
%\hline
%&&\\
%&&\\
%\hline
%\end{tabular}
%\end{table*}

%%%%%%%%%%% The bibliography starts:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  ios1.bst will be used to                               %%
%%  create a .BBL file for submission.                     %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\nocite{*} 
% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{ios1}           % Style BST file.
\bibliography{scaling}        % Bibliography file (usually '*.bib')

% or include bibliography directly:
%\begin{thebibliography}{0}
%\bibitem{r1} F. Author, Information about cited object.
%
%\bibitem{r2} S. Author and T. Author, Information about cited object.
%\end{thebibliography}

\end{document}
